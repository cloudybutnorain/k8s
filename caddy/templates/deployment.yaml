apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ $.Release.Name }}
  labels:
    app: {{ $.Chart.Name }}
    release: {{ $.Release.Name }}
spec:
  replicas: 1
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: {{ $.Chart.Name }}
      release: {{ $.Release.Name }}
  template:
    metadata:
      labels:
        app: {{ $.Chart.Name }}
        release: {{ $.Release.Name }}
      annotations:
        checksum/config: {{ include "caddy.json" . | sha256sum }}
        prometheus.io/scrape: "true"
        prometheus.io/path: /metrics
        prometheus.io/port: "2019"
    spec:
      terminationGracePeriodSeconds: 10
      serviceAccountName: {{ $.Release.Name }}
      containers:
      - name: caddy
        image: pickledish/caddy:{{ $.Chart.AppVersion }}
        command: ["caddy"]
        args: ["run", "--config", "/etc/caddy/caddy.json"]
        # below, we claim ports 80 443 and 517 on the entire node
        # so, implicitly, only one caddy pod per node using this
        # also means we need no service.yaml, as this does that job
        ports:
        - name: {{ $.Release.Name }}-http
          containerPort: 80
          protocol: TCP
          hostPort: 80
        - name: {{ $.Release.Name }}-https
          containerPort: 443
          protocol: TCP
          hostPort: 443
        - name: {{ $.Release.Name }}-tcp
          containerPort: 517
          protocol: TCP
          hostPort: 517
          # the prom-scraping port, no host port for this one
        - name: {{ $.Release.Name }}-metrics
          containerPort: 2019
          protocol: TCP
        volumeMounts:
        - name: {{ $.Release.Name }}-config
          mountPath: /etc/caddy/
      volumes:
      - name: {{ $.Release.Name }}-config
        configMap:
          name: {{ $.Release.Name }}
