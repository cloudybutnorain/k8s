apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ $.Release.Name }}
  labels:
    app: {{ $.Chart.Name }}
spec:
  replicas: 1
  minReadySeconds: 10
  selector:
    matchLabels:
      app: {{ $.Chart.Name }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include "caddy.config" . | sha256sum }}
      labels:
        app: {{ $.Chart.Name }}
    spec:
      terminationGracePeriodSeconds: 10
      serviceAccountName: {{ $.Release.Name }}
      containers:
      - name: caddy
        image: pickledish/caddy:{{ $.Chart.AppVersion }}
        command: ["caddy"]
        args: ["run", "--config", "/etc/caddy/caddy.json"]
        # below, we claim ports 80 443 and 517 on the entire node
        # so, implicitly, only one caddy pod per node using this
        # also means we need no service.yaml, as this does that job
        ports:
        - name: {{ $.Release.Name }}-http
          containerPort: 80
          protocol: TCP
          hostPort: 80
        - name: {{ $.Release.Name }}-https
          containerPort: 443
          protocol: TCP
          hostPort: 443
        - name: {{ $.Release.Name }}-tcp
          containerPort: 517
          protocol: TCP
          hostPort: 517
        resources: {}
        volumeMounts:
        - name: {{ $.Release.Name }}-data
          mountPath: /var/lib/caddy/.local/share/caddy/
        - name: {{ $.Release.Name }}-config-volume
          mountPath: /etc/caddy/
      volumes:
      - name: {{ $.Release.Name }}-config-volume
        configMap:
          name: {{ $.Release.Name }}-config
  # this is why it needs to be a statefulset, the persistent path for certs
  # once it is set up with redis/s3/whatever it can be a deployment again
  volumeClaimTemplates:
  - metadata:
      name: {{ $.Release.Name }}-data
    spec:
      accessModes:
      - "ReadWriteOnce"
      storageClassName: "local-path"
      resources:
        requests:
          storage: 1Gi
